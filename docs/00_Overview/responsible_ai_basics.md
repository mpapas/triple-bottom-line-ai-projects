# Responsible AI Basics

**Responsible AI (RAI)** refers to the design, development, and deployment of AI systems in ways that are **ethical, transparent, and aligned with human values**.

While different organizations use slightly different terminology, the following six principles are broadly recognized across frameworks such as the **OECD AI Principles**, **NIST AI Risk Management Framework**, and **ISO/IEC 42001**:

| Principle | Description | PM’s Responsibility |
|------------|--------------|----------------------|
| **Fairness** | AI systems should treat individuals and groups equitably and without bias. | Ensure diversity in training data and include bias testing in QA. |
| **Accountability** | Humans remain responsible for outcomes of AI decisions. | Define ownership and escalation paths for model decisions. |
| **Transparency** | AI decisions should be explainable and traceable. | Require documentation of model assumptions and limitations. |
| **Reliability & Safety** | AI systems should perform consistently and safely. | Add performance validation and human-in-the-loop checkpoints. |
| **Privacy & Security** | Data should be handled securely and respectfully. | Enforce privacy-by-design and secure data management plans. |
| **Sustainability** | AI should be energy- and resource-efficient. | Track compute emissions and select low-carbon hosting options. |

Project Managers operationalize these principles by embedding them into **project plans, risk registers, governance reviews, and acceptance criteria**.  
Responsible AI is not just a technical goal — it’s a project management discipline.
